# âš™ï¸ Container Device Interface (CDI)

### ğŸŒ Quâ€™est-ce que CDI ?

La **Container Device Interface (CDI)** est une **spÃ©cification** conÃ§ue pour **standardiser** la maniÃ¨re dont les pÃ©riphÃ©riques matÃ©riels (tels que :

* ğŸ® GPU,
* âš¡ FPGA,
* ğŸ”¬ accÃ©lÃ©rateurs matÃ©riels divers)

sont exposÃ©s et utilisÃ©s par des conteneurs.

#### ğŸ¯ Objectifs de CDI :

* Fournir un mÃ©canisme **cohÃ©rent et sÃ©curisÃ©** pour utiliser du matÃ©riel dans des environnements conteneurisÃ©s.
* Ã‰liminer la complexitÃ© liÃ©e aux **configurations spÃ©cifiques aux pÃ©riphÃ©riques**.
* Rendre lâ€™intÃ©gration de matÃ©riel dans Docker plus **portable** et **prÃ©visible**.

***

### â• Que permet CDI en plus ?

Outre lâ€™accÃ¨s du conteneur au **nÅ“ud de pÃ©riphÃ©rique** (_device node_), CDI permet aussi de spÃ©cifier :

* ğŸ”§ des **variables dâ€™environnement** associÃ©es au pÃ©riphÃ©rique,
* ğŸ“‚ des **montages de lâ€™hÃ´te** (par ex. des bibliothÃ¨ques partagÃ©es `*.so`),
* âš™ï¸ des **hooks exÃ©cutables** (scripts ou binaires dÃ©clenchÃ©s automatiquement).

ğŸ‘‰ En rÃ©sumÃ© : CDI ne fait pas que Â« brancher Â» le pÃ©riphÃ©rique, il fournit aussi son contexte dâ€™exÃ©cution complet.

***

## ğŸš€ Mise en route

### ğŸ“Œ PrÃ©requis

Pour commencer Ã  utiliser CDI, il faut :

* **Docker v27+** installÃ© avec CDI configurÃ©,
* **Buildx v0.22+** ou plus rÃ©cent.

***

### ğŸ“ CrÃ©ation des spÃ©cifications de pÃ©riphÃ©riques

Les spÃ©cifications CDI doivent Ãªtre dÃ©finies via des fichiers **JSON** ou **YAML**, et placÃ©es dans lâ€™un des emplacements suivants :

* `/etc/cdi`
* `/var/run/cdi`
* `/etc/buildkit/cdi`

***

### ğŸ“‘ Remarques importantes

#### ğŸ”§ Modifier lâ€™emplacement des specs

* Si vous utilisez **BuildKit directement**, vous pouvez changer lâ€™emplacement avec lâ€™option **`specDirs`** dans la section **`cdi`** du fichier `buildkitd.toml`.

#### ğŸ³ Si vous utilisez Docker Daemon avec le driver `docker`

* Consultez la doc **Configure CDI devices** pour configurer correctement les pÃ©riphÃ©riques CDI.

#### ğŸ’» Cas particulier WSL (Windows Subsystem for Linux)

* Si vous crÃ©ez un **builder de conteneurs sur WSL** :
  * installez **Docker Desktop**,
  * activez la **paravirtualisation GPU de WSL 2**,
  * utilisez **Buildx v0.27+** (nÃ©cessaire pour monter les bibliothÃ¨ques WSL dans le conteneur).

## ğŸ› ï¸ Construire avec une spÃ©cification CDI simple

### ğŸ¯ Exemple de base

On va dÃ©marrer avec une spÃ©cification CDI trÃ¨s simple, qui injecte une **variable dâ€™environnement** dans lâ€™environnement de build.\
On Ã©crit cette spÃ©cification dans le fichier **`/etc/cdi/foo.yaml`** :

```yaml
/etc/cdi/foo.yaml

cdiVersion: "0.6.0"
kind: "vendor1.com/device"
devices:
- name: foo
  containerEdits:
    env:
    - FOO=injected
```

***

### ğŸ” VÃ©rifier la dÃ©tection du pÃ©riphÃ©rique

Ensuite, inspectons le builder par dÃ©faut pour vÃ©rifier que **`vendor1.com/device`** est bien dÃ©tectÃ© :

```bash
docker buildx inspect
```

Extrait pertinent de la sortie :

```
Devices:
 Name:                  vendor1.com/device=foo
 Automatically allowed: false
```

ğŸ‘‰ Le pÃ©riphÃ©rique est bien dÃ©tectÃ© (`foo`), mais **il nâ€™est pas autorisÃ© automatiquement**.

***

### ğŸ³ CrÃ©er un Dockerfile qui utilise ce pÃ©riphÃ©rique

```dockerfile
# syntax=docker/dockerfile:1-labs
FROM busybox
RUN --device=vendor1.com/device \
  env | grep ^FOO=
```

* Ici, on utilise la commande **`RUN --device`** pour demander un pÃ©riphÃ©rique CDI.
* Le pÃ©riphÃ©rique demandÃ© est `vendor1.com/device`.
* Dans notre fichier `/etc/cdi/foo.yaml`, le premier pÃ©riphÃ©rique est `foo`, donc câ€™est lui qui sera utilisÃ©.

âš ï¸ **Note importante** :\
La commande `RUN --device` nâ€™est disponible que dans le **canal labs** depuis `Dockerfile frontend v1.14.0-labs`.\
Elle nâ€™est **pas encore disponible dans la syntaxe stable**.

***

### ğŸš§ Premier essai de build

```bash
docker buildx build .
```

RÃ©sultat :

```
ERROR: failed to build: failed to solve: failed to load LLB: device vendor1.com/device=foo is requested by the build but not allowed
```

ğŸ‘‰ Lâ€™erreur se produit car, comme vu plus haut, le pÃ©riphÃ©rique `vendor1.com/device=foo` nâ€™est pas automatiquement autorisÃ©.

***

### âœ… Autoriser explicitement le pÃ©riphÃ©rique

Deux solutions possibles :

#### 1. Avec le flag `--allow`

Autoriser le pÃ©riphÃ©rique manuellement lors du build :

```bash
docker buildx build --allow device .
```

#### 2. Avec une annotation dans la spec CDI

Modifier le fichier `/etc/cdi/foo.yaml` pour autoriser automatiquement le pÃ©riphÃ©rique pour tous les builds :

```yaml
cdiVersion: "0.6.0"
kind: "vendor1.com/device"
devices:
- name: foo
  containerEdits:
    env:
    - FOO=injected
annotations:
  org.mobyproject.buildkit.device.autoallow: true
```

***

### ğŸš€ Relancer le build avec autorisation

```bash
docker buildx build --progress=plain --allow device .
```

Sortie :

```
7 [2/2] RUN --device=vendor1.com/device   env | grep ^FOO=
7 0.155 FOO=injected
7 DONE 0.2s
```

ğŸ‘‰ Cette fois, le build rÃ©ussit ğŸ‰\
ğŸ‘‰ La variable dâ€™environnement **`FOO=injected`** a bien Ã©tÃ© injectÃ©e dans lâ€™environnement de build, comme spÃ©cifiÃ© dans la spec CDI.

## âš¡ Mettre en place un builder de conteneur avec support GPU (NVIDIA)

### ğŸ¯ Contexte

Depuis **Buildx v0.22**, quand vous crÃ©ez un **builder de type conteneur**, une requÃªte GPU est **ajoutÃ©e automatiquement** si le systÃ¨me hÃ´te dispose des **drivers GPU installÃ©s dans le noyau**.

ğŸ‘‰ Cela fonctionne un peu comme si vous lanciez un conteneur avec :

```bash
docker run --gpus=all ...
```

***

### ğŸ“ Note importante

Lâ€™image officielle de **BuildKit** est basÃ©e sur **Alpine**, qui **ne supporte pas les drivers NVIDIA**.

ğŸ‘‰ Pour cette raison, une image spÃ©ciale a Ã©tÃ© crÃ©Ã©e :

* basÃ©e sur **Ubuntu**,
* intÃ©grant les bibliothÃ¨ques clients NVIDIA,
* gÃ©nÃ©rant automatiquement une **spÃ©cification CDI GPU** si un pÃ©riphÃ©rique est demandÃ© lors dâ€™un build.

ğŸ“¦ Cette image est hÃ©bergÃ©e temporairement sur Docker Hub :

```
crazymax/buildkit:v0.23.2-ubuntu-nvidia
```

***

### ğŸš€ Ã‰tape 1 : CrÃ©er un builder GPU

CrÃ©ons un builder nommÃ© **gpubuilder** avec lâ€™image spÃ©ciale BuildKit :

```bash
docker buildx create \
  --name gpubuilder \
  --driver-opt "image=crazymax/buildkit:v0.23.2-ubuntu-nvidia" \
  --bootstrap
```

Sortie (abrÃ©gÃ©e) :

```
1 pulling image crazymax/buildkit:v0.23.2-ubuntu-nvidia
1 creating container buildx_buildkit_gpubuilder0
gpubuilder
```

ğŸ‘‰ Le builder est maintenant crÃ©Ã© et initialisÃ©.

***

### ğŸ” Ã‰tape 2 : Inspecter le builder

```bash
docker buildx inspect gpubuilder
```

Sortie pertinente :

```
Devices:
 Name:      nvidia.com/gpu
 On-Demand: true
```

ğŸ‘‰ Cela signifie que le **vendor `nvidia.com/gpu`** est bien dÃ©tectÃ© comme pÃ©riphÃ©rique.\
ğŸ‘‰ Donc les **drivers NVIDIA** ont Ã©tÃ© trouvÃ©s avec succÃ¨s sur lâ€™hÃ´te. ğŸ‰

***

### ğŸ–¥ï¸ Ã‰tape 3 (optionnelle) : VÃ©rifier avec `nvidia-smi`

Vous pouvez vÃ©rifier que le GPU est bien disponible dans le conteneur builder :

```bash
docker exec -it buildx_buildkit_gpubuilder0 nvidia-smi -L
```

Exemple de sortie :

```
GPU 0: Tesla T4 (UUID: GPU-6cf00fa7-59ac-16f2-3e83-d24ccdc56f84)
```

ğŸ‘‰ Confirmation : la carte GPU est bien visible et utilisable dans le builder.

***

## âœ… RÃ©sumÃ©

* CrÃ©ation dâ€™un builder **GPU-aware** grÃ¢ce Ã  lâ€™image Ubuntu spÃ©ciale.
* DÃ©tection automatique du pÃ©riphÃ©rique **`nvidia.com/gpu`**.
* VÃ©rification possible avec `nvidia-smi`.

## âš¡ Construire avec support GPU

### ğŸ¯ Objectif

Nous allons crÃ©er un build simple qui exploite un **GPU NVIDIA** via CDI (Container Device Interface).

***

### ğŸ“ Dockerfile dâ€™exemple

```dockerfile
# syntax=docker/dockerfile:1-labs
FROM ubuntu
RUN --device=nvidia.com/gpu nvidia-smi -L
```

ğŸ‘‰ Ici :

* La directive **`RUN --device=nvidia.com/gpu`** demande Ã  BuildKit dâ€™utiliser le pÃ©riphÃ©rique GPU dÃ©tectÃ© (`nvidia.com/gpu`).
* La commande **`nvidia-smi -L`** liste les GPU disponibles avec leur UUID.

***

### ğŸš€ Lancer le build avec le builder GPU

On utilise le builder **gpubuilder** crÃ©Ã© prÃ©cÃ©demment :

```bash
docker buildx --builder gpubuilder build --progress=plain .
```

Sortie (abrÃ©gÃ©e) :

```
7 preparing device nvidia.com/gpu
...
7 17.80 time="2025-04-15T08:58:16Z" level=info msg="Generated CDI spec with version 0.8.0"
...
8 [2/2] RUN --device=nvidia.com/gpu nvidia-smi -L
8 0.527 GPU 0: Tesla T4 (UUID: GPU-6cf00fa7-59ac-16f2-3e83-d24ccdc56f84)
```

#### âœ… Analyse :

* Lâ€™Ã©tape **7** prÃ©pare le pÃ©riphÃ©rique GPU â†’ installation des bibliothÃ¨ques clientes NVIDIA + gÃ©nÃ©ration automatique de la spÃ©cification CDI.
* Lâ€™Ã©tape **8** exÃ©cute `nvidia-smi -L` dans le conteneur â†’ le GPU est bien reconnu (ex. **Tesla T4**).

***

### ğŸ” VÃ©rifier la spÃ©cification CDI gÃ©nÃ©rÃ©e

On peut consulter le fichier gÃ©nÃ©rÃ© dans le builder :

```bash
docker exec -it buildx_buildkit_gpubuilder0 cat /etc/cdi/nvidia.yaml
```

Exemple de contenu (simplifiÃ©, instance EC2 `g4dn.xlarge`) :

```yaml
cdiVersion: 0.6.0
containerEdits:
  deviceNodes:
  - path: /dev/nvidia-modeset
  - path: /dev/nvidia-uvm
  - path: /dev/nvidiactl
  env:
  - NVIDIA_VISIBLE_DEVICES=void
  hooks:
  - args:
    - nvidia-cdi-hook
    - enable-cuda-compat
    - --host-driver-version=570.133.20
    hookName: createContainer
    path: /usr/bin/nvidia-cdi-hook
  mounts:
  - containerPath: /usr/bin/nvidia-smi
    hostPath: /usr/bin/nvidia-smi
    options:
    - ro
    - bind
devices:
- containerEdits:
    deviceNodes:
    - path: /dev/nvidia0
  name: "0"
- containerEdits:
    deviceNodes:
    - path: /dev/nvidia0
  name: GPU-6cf00fa7-59ac-16f2-3e83-d24ccdc56f84
- containerEdits:
    deviceNodes:
    - path: /dev/nvidia0
  name: all
kind: nvidia.com/gpu
```

ğŸ‘‰ Cette spÃ©cification CDI :

* Monte automatiquement les **bibliothÃ¨ques NVIDIA** (`libcuda`, `libnvidia-ml`, etc.),
* Ajoute les **sockets et exÃ©cutables nÃ©cessaires** (`nvidia-smi`, `nvidia-persistenced`â€¦),
* GÃ¨re les **hooks** (scripts exÃ©cutÃ©s au dÃ©marrage du conteneur) pour activer certaines fonctionnalitÃ©s,
* Expose les **devices `/dev/nvidia*`**.

***

## ğŸ‰ FÃ©licitations

Vous venez de rÃ©aliser votre **premier build utilisant un GPU NVIDIA avec BuildKit et CDI** !
